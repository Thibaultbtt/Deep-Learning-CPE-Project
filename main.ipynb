{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle pour la détermination du type de véhicule\n",
    "\n",
    "### 1. Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop provisoire des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def crop_images (dossier_crops, dossier_coordonnees, dossier_images) :\n",
    "    # Obtenir les listes triées des fichiers dans les dossiers\n",
    "    fichiers_images = sorted(os.listdir(dossier_images))\n",
    "    fichiers_coordonnees = sorted(os.listdir(dossier_coordonnees))\n",
    "\n",
    "    # Vérifier si les deux dossiers ont le même nombre de fichiers\n",
    "    if len(fichiers_images) != len(fichiers_coordonnees):\n",
    "        print(\"Le nombre de fichiers dans les deux dossiers ne correspond pas.\")\n",
    "        exit()\n",
    "\n",
    "    # Parcours des images et fichiers de coordonnées\n",
    "    for index, image_file in enumerate(fichiers_images):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(dossier_images, image_file)\n",
    "            coord_file = os.path.join(dossier_coordonnees, fichiers_coordonnees[index])\n",
    "            \n",
    "            # Charger l'image\n",
    "            with Image.open(image_path) as img:\n",
    "                img_width, img_height = img.size  # Taille de l'image\n",
    "                # Lire les coordonnées\n",
    "                with open(coord_file, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                    for i, line in enumerate(lines):\n",
    "                        data = line.strip().split()\n",
    "                        \n",
    "                        # Vérifier la validité des données\n",
    "                        if len(data) < 9:\n",
    "                            print(f\"Coordonnées invalides dans {coord_file}, ligne {i + 1}\")\n",
    "                            continue\n",
    "\n",
    "                        img_type = data[-2]\n",
    "\n",
    "                        # Calcul des coordonnées absolues du rectangle (x_min, y_min, x_max, y_max)\n",
    "                        x_min = int(round(float(data[2])))\n",
    "                        y_min = int(round(float(data[1])))\n",
    "                        x_max = int(round(float(data[0])))\n",
    "                        y_max = int(round(float(data[5])))\n",
    "\n",
    "                        # Découper l'image\n",
    "                        cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
    "                        \n",
    "                        # Sauvegarder l'image cropée\n",
    "                        crop_file_name = f\"{img_type}_{index + 1}_{i + 1}.jpg\"\n",
    "                        crop_path = os.path.join(dossier_crops, crop_file_name)\n",
    "                        cropped_img.save(crop_path, \"JPEG\")\n",
    "                        #print(f\"Image cropée enregistrée : {crop_path}\")\n",
    "                        #Nom d'origine : {image_file}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : Niveau de gris sur tout le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "#change_to_gray_input_folder = './result'\n",
    "#change_to_gray_output_folder = './gray_images'\n",
    "\n",
    "def change_to_gray (input_folder, output_folder) :\n",
    "    \"\"\"\n",
    "    Transforme une image à l'origine en couleur en niveaux de gris\n",
    "    \"\"\"\n",
    "    for image_name in os.listdir(input_folder) :\n",
    "        img = cv2.imread(f'./cropped_images/{image_name}')\n",
    "        grayFrame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        output_path = os.path.join(output_folder, image_name)\n",
    "        cv2.imwrite(output_path, grayFrame)\n",
    "    #print(f'End of the process Color to Gray.\\nFind the images in the folder {output_folder}')\n",
    "\n",
    "#change_to_gray (change_to_gray_input_folder, change_to_gray_output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : ajout de bruit gaussien sur les images pour simuler une caméra de mauvaise qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def add_gaussian_noise(img, mean=0, std=25):\n",
    "    \"\"\"\n",
    "    Ajoute un bruit gaussien à une image.\n",
    "    \"\"\"\n",
    "    # Convertir l'image en tableau numpy\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "\n",
    "    # Générer du bruit gaussien\n",
    "    std = 10\n",
    "    noise = np.random.normal(mean, std, img_array.shape)\n",
    "    noisy_img = img_array + noise\n",
    "\n",
    "    # Limiter les valeurs entre 0 et 255\n",
    "    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
    "    return noisy_img\n",
    "\n",
    "def process_images_with_effects(source_dir, target_dir, apply_blur=True, apply_noise=True):\n",
    "    \"\"\"\n",
    "    Applique flou et bruit aux images redimensionnées.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            save_path = os.path.join(target_dir, filename)\n",
    "\n",
    "            # Charger l'image\n",
    "            with Image.open(file_path) as img:\n",
    "                # Convertir en BGR pour OpenCV\n",
    "                img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Appliquer bruit gaussien\n",
    "                if apply_noise and random.random() > 0.3:  # 50% de chances d'ajouter du bruit\n",
    "                    img_cv = add_gaussian_noise(img_cv, mean=0, std=random.randint(15, 40))\n",
    "\n",
    "                # Convertir en RGB et sauvegarder\n",
    "                final_img = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "                final_img.save(save_path)\n",
    "                #print(f\"Processed and saved: {save_path}\")\n",
    "\n",
    "# # Répertoires\n",
    "# source_directory = \"./gray_images\"  # Changez avec votre chemin\n",
    "# target_directory = \"./noise_images\"  # Répertoire cible\n",
    "\n",
    "# # Appliquer flou et bruit\n",
    "# process_images_with_effects(source_directory, target_directory, apply_blur=True, apply_noise=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : Redimensionnement pour que les images soient environ de la taille de la médiane des tailles des exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def calculate_median_size(image_paths):\n",
    "    \"\"\"Calcul la médiane des tailles (largeur, hauteur) des images.\"\"\"\n",
    "    dimensions = []\n",
    "    for path in image_paths:\n",
    "        with Image.open(path) as img:\n",
    "            dimensions.append(img.size)  # (width, height)\n",
    "    dimensions = np.array(dimensions)\n",
    "    median_width = int(np.median(dimensions[:, 0]))\n",
    "    median_height = int(np.median(dimensions[:, 1]))\n",
    "    return median_width, median_height\n",
    "\n",
    "def add_random_variation(size, variation_percent=0.2):\n",
    "    \"\"\"\n",
    "    Ajoute une variation aléatoire à une taille donnée.\n",
    "    \"\"\"\n",
    "    factor = 1 + random.uniform(-variation_percent, variation_percent)\n",
    "    return int(size * factor)\n",
    "\n",
    "def resize_image_with_aspect_ratio(img, target_width, target_height):\n",
    "    \"\"\"\n",
    "    Redimensionne une image en conservant le ratio d'aspect, en s'adaptant à la taille cible.\n",
    "    \"\"\"\n",
    "    original_width, original_height = img.size\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    # Ajuster la largeur et la hauteur pour conserver le ratio\n",
    "    if target_width / target_height > aspect_ratio:\n",
    "        # Ajuste par la hauteur\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    else:\n",
    "        # Ajuste par la largeur\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "    try :\n",
    "        return img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    except :\n",
    "        return img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "def resize_images(source_dir, target_dir, median_size, variation_percent=0.2):\n",
    "    \"\"\"\n",
    "    Redimensionne les images en conservant leur ratio d'aspect, avec des tailles légèrement différentes.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            with Image.open(file_path) as img:\n",
    "                # Ajouter une variation aléatoire aux dimensions médianes\n",
    "                target_width = add_random_variation(median_size[0], variation_percent)\n",
    "                target_height = add_random_variation(median_size[1], variation_percent)\n",
    "\n",
    "                # Redimensionner en conservant le ratio d'aspect\n",
    "                resized_img = resize_image_with_aspect_ratio(img, target_width, target_height)\n",
    "\n",
    "                # Sauvegarder l'image redimensionnée\n",
    "                save_path = os.path.join(target_dir, filename)\n",
    "                resized_img.save(save_path)\n",
    "                #print(f\"Resized and saved: {save_path} -> {resized_img.size}\")\n",
    "\n",
    "# # Répertoires\n",
    "# source_directory = \"./noise_images\"  # Changez avec votre chemin\n",
    "# target_directory = \"./resized_images\"  # Répertoire cible\n",
    "\n",
    "# dimension_images = \"./AlgorithmePreProcess/dimension_images\"\n",
    "\n",
    "# # Images avec tailles définies\n",
    "# defined_images = [os.path.join(dimension_images, f) for f in os.listdir(dimension_images)\n",
    "#                   if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "# # Calcul de la taille médiane\n",
    "# median_size = calculate_median_size(defined_images)\n",
    "\n",
    "# # Redimensionnement des images avec une variation aléatoire de ±20 %\n",
    "# resize_images(source_directory, target_directory, median_size, variation_percent=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : UpSampling en retournant les images aléatoirement\n",
    "Tous les exemples uniques de rotation/flip d'image :\n",
    "\n",
    "| Rotation (k) | Flip Horizontal | Flip Vertical |\n",
    "|--------------|-----------------|---------------|\n",
    "| 0            | False           | False         |\n",
    "| 0            | True            | False         |\n",
    "| 0            | False           | True          |\n",
    "| 1            | False           | False         |\n",
    "| 1            | True            | False         |\n",
    "| 2            | False           | False         |\n",
    "| 2            | True            | False         |\n",
    "| 3            | False           | False         |\n",
    "| 3            | True            | False         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# def random_rotate_image(opencv_img):\n",
    "#     img = tf.convert_to_tensor(opencv_img, dtype=tf.float32)\n",
    "#     k = random.randint(1, 3)  # 1 = 90°, 2 = 180°, 3 = 270°\n",
    "#     img = tf.image.rot90(img, k=k)\n",
    "#     if k != 4:\n",
    "#         img = tf.image.random_flip_left_right(img)\n",
    "#         img = tf.image.random_flip_up_down(img)\n",
    "\n",
    "#     img_rotated = img.numpy().astype(np.uint8)\n",
    "#     return img_rotated\n",
    "\n",
    "unique_combinations = [\n",
    "    # (0, False, False), ==> Image de base : (nb de flip à 90°, mirroir horizontal, mirroir veritcal)\n",
    "    (0, True, False),\n",
    "    (0, False, True),\n",
    "    (1, False, False),\n",
    "    (1, True, False),\n",
    "    (2, False, False),\n",
    "    (2, True, False),\n",
    "    (3, False, False),\n",
    "    (3, True, False)\n",
    "]\n",
    "\n",
    "def apply_transformations(img, rotation_count, flip_horizontal, flip_vertical):\n",
    "    \"\"\"\n",
    "    Applique une transformation à l'image selon le nombre de rotations et les flips.\n",
    "    \"\"\"\n",
    "    img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "\n",
    "    img = tf.image.rot90(img, k=rotation_count)\n",
    "\n",
    "    # Appliquer les flips\n",
    "    if flip_horizontal:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    if flip_vertical:\n",
    "        img = tf.image.flip_up_down(img)\n",
    "\n",
    "    img_rotated = img.numpy().astype(np.uint8)\n",
    "    return img_rotated\n",
    "\n",
    "def rotate_images (output_folder) :\n",
    "    for image_name in os.listdir('./resized_images') :\n",
    "        img = cv2.imread(f'./resized_images/{image_name}')\n",
    "\n",
    "        # On choisit deux transformations aléatoires\n",
    "        random_choice1 = random.choice(unique_combinations)\n",
    "        random_choice2 = random.choice(unique_combinations)\n",
    "\n",
    "        # On vérifie que les choix sont différents\n",
    "        while random_choice1 == random_choice2:\n",
    "            random_choice2 = random.choice(unique_combinations)\n",
    "\n",
    "        rotation_count1, flip_horizontal1, flip_vertical1 = random_choice1\n",
    "        rotation_count2, flip_horizontal2, flip_vertical2 = random_choice2\n",
    "\n",
    "        rotations_list = [apply_transformations(img, rotation_count1, flip_horizontal1, flip_vertical1), \n",
    "                        apply_transformations(img, rotation_count2, flip_horizontal2, flip_vertical2)]\n",
    "        \n",
    "\n",
    "        img_name = os.path.splitext(image_name)[0]\n",
    "        category = img_name.split(\"_\")[0]\n",
    "        output_path = os.path.join(output_folder + f'{category}/', f\"{img_name}.png\")\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "        for i, rotated_image in enumerate(rotations_list) :\n",
    "            output_image_name = f\"{img_name}_{i+1}.png\"\n",
    "            output_path = os.path.join(output_folder + f'{category}/', output_image_name)\n",
    "            cv2.imwrite(output_path, rotated_image)\n",
    "\n",
    "        # for i, rotated_image in enumerate(rotations_list) :\n",
    "        #     img_name = os.path.splitext(image_name)[0]\n",
    "        #     category = img_name.split(\"_\")[0]\n",
    "        #     output_image_name = f\"{img_name}_{i+1}.png\"\n",
    "        #     output_path = os.path.join(output_folder + f'{category}/', output_image_name)\n",
    "        #     cv2.imwrite(output_path, rotated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread(f'./resized_images/cars_2_1.jpg')\n",
    "# img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "# img = tf.image.rot90(img, k=2)\n",
    "# img = tf.image.random_flip_up_down(img)\n",
    "# img_rotated = img.numpy().astype(np.uint8)\n",
    "# output_folder = './TEST_ROTATION'\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# output_image_name = f\"car_2_rotated_flip.png\"\n",
    "# output_path = os.path.join(output_folder, output_image_name)\n",
    "# cv2.imwrite(output_path, img_rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version finale de la préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None  # Supprime complètement la limite\n",
    "\n",
    "def prepare_data (output_folder , mode) :\n",
    "    print(f'Beginning of the preparation process for {mode} data.\\n')\n",
    "\n",
    "    # Chemins des dossiers\n",
    "    dossier_images = f\"./dataset_part_2/{mode}/images\"  # Chemin vers le dossier contenant les images\n",
    "    dossier_coordonnees = f\"./dataset_part_2/{mode}/labelTxt\"  # Chemin vers le dossier contenant les fichiers de coordonnées\n",
    "    dossier_crops = \"./cropped_images\"  # Dossier de sortie pour les images cropées\n",
    "\n",
    "    #os.makedirs(output_folder, exist_ok=True)\n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(dossier_crops, exist_ok=True)\n",
    "    crop_images (dossier_crops, dossier_coordonnees, dossier_images)\n",
    "    ### la partie au dessus sert à croper provisoirement. Elle sera à enlever plus tard.\n",
    "\n",
    "    os.makedirs(\"./gray_images\", exist_ok=True)\n",
    "    change_to_gray_input_folder = './cropped_images'\n",
    "    change_to_gray_output_folder = './gray_images'\n",
    "    change_to_gray (change_to_gray_input_folder, change_to_gray_output_folder)\n",
    "    shutil.rmtree('./cropped_images')\n",
    "    print(f'End of the process Color to Gray')\n",
    "\n",
    "    # Répertoires\n",
    "    os.makedirs(\"./noise_images\", exist_ok=True)\n",
    "    source_directory = \"./gray_images\"\n",
    "    target_directory = \"./noise_images\"\n",
    "\n",
    "    # Appliquer flou et bruit\n",
    "    process_images_with_effects(source_directory, target_directory, apply_blur=True, apply_noise=True)\n",
    "    print(f'End of the process Add Effects on Images')\n",
    "    shutil.rmtree('./gray_images')\n",
    "\n",
    "    # Répertoires\n",
    "    os.makedirs(\"./resized_images\", exist_ok=True)\n",
    "    source_directory = \"./noise_images\"\n",
    "    target_directory = \"./resized_images\"\n",
    "    dimension_images = \"./AlgorithmePreProcess/dimension_images\"\n",
    "\n",
    "    # Images avec tailles définies\n",
    "    defined_images = [os.path.join(dimension_images, f) for f in os.listdir(dimension_images)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "    # Calcul de la taille médiane\n",
    "    median_size = calculate_median_size(defined_images)\n",
    "\n",
    "    # Redimensionnement des images avec une variation aléatoire de ±20 %\n",
    "    resize_images(source_directory, target_directory, median_size, variation_percent=0.2)\n",
    "    print(f'End of the process Resize Images')\n",
    "    shutil.rmtree('./noise_images')\n",
    "\n",
    "    # Génération d'images tournées dans des sens différents\n",
    "    rotate_images (output_folder)\n",
    "    print(f'End of the process Random Turn Images')\n",
    "    shutil.rmtree('./resized_images')\n",
    "\n",
    "    print(f'\\nEnd of the Preparation process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of the preparation process for train data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rapha\\AppData\\Local\\Temp\\ipykernel_33556\\1998891962.py:42: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  return img.resize((new_width, new_height), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n",
      "Beginning of the preparation process for test data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n",
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n",
      "Beginning of the preparation process for validation data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n",
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n"
     ]
    }
   ],
   "source": [
    "train_output_folder = './train_prepared_images/'\n",
    "test_output_folder = './test_prepared_images/'\n",
    "validation_output_folder = './validation_prepared_images/'\n",
    "\n",
    "shutil.rmtree('./train_prepared_images')\n",
    "shutil.rmtree('./test_prepared_images')\n",
    "shutil.rmtree('./validation_prepared_images')\n",
    "\n",
    "os.mkdir('./train_prepared_images')\n",
    "os.mkdir('./train_prepared_images/cars')\n",
    "os.mkdir('./train_prepared_images/truck')\n",
    "\n",
    "os.mkdir('./test_prepared_images')\n",
    "os.mkdir('./test_prepared_images/cars')\n",
    "os.mkdir('./test_prepared_images/truck')\n",
    "\n",
    "os.mkdir('./validation_prepared_images')\n",
    "os.mkdir('./validation_prepared_images/cars')\n",
    "os.mkdir('./validation_prepared_images/truck')\n",
    "\n",
    "prepare_data (train_output_folder , 'train')\n",
    "prepare_data (test_output_folder , 'test')\n",
    "prepare_data (validation_output_folder , 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2292 files belonging to 2 classes.\n",
      "Found 666 files belonging to 2 classes.\n",
      "Found 327 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Définition des chemins pour les fichiers image\n",
    "train_dir = \"./train_prepared_images\"\n",
    "val_dir = \"./validation_prepared_images\"\n",
    "test_dir = \"./test_prepared_images\"\n",
    "\n",
    "# Définition des constantes \n",
    "dimension_images = \"./AlgorithmePreProcess/dimension_images\"\n",
    "\n",
    "# Images avec tailles définies\n",
    "defined_images = [os.path.join(dimension_images, f) for f in os.listdir(dimension_images)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "img_width,img_height = calculate_median_size(defined_images)\n",
    "batch_size = 32\n",
    "# img_height = 224  # Hauteur de l'image (modifiable selon vos besoins)\n",
    "# img_width = 224 \n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_63 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_58 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_64 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_59 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_65 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_60 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_18 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m393,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_18 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">486,913</span> (1.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m486,913\u001b[0m (1.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">486,913</span> (1.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m486,913\u001b[0m (1.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True) # Keep best model instead of last\n",
    "early_stopping_cb = EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True) # Prevent overfitting\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(img_width, img_height, 1)),\n",
    "        layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'),#, input_shape=(img_width, img_height, 1)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),#, input_shape=(img_width, img_height, 1)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),#, input_shape=(img_width, img_height, 1)),\n",
    "        layers.MaxPooling2D((2,2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        #layers.Dense(len(train_dataset.class_names), activation='softmax')\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#                 loss='sparse_categorical_crossentropy',\n",
    "#                 metrics=['accuracy'])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    # Entrée\n",
    "    layers.Input(shape=(img_width, img_height, 1)),\n",
    "\n",
    "    # Bloc convolutionnel 1\n",
    "    layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'),\n",
    "    layers.BatchNormalization(),  # Ajout d'une normalisation\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Bloc convolutionnel 2\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Bloc convolutionnel 3\n",
    "    layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # Bloc convolutionnel 4 (ajouté pour extraire davantage de caractéristiques complexes)\n",
    "    layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # Bloc convolutionnel 5 (couche supplémentaire avec une taille de noyau différente)\n",
    "    layers.Conv2D(filters=512, kernel_size=(2, 2), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "\n",
    "    # Aplatissement des caractéristiques\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Couches denses\n",
    "    layers.Dense(512, activation='relu'),  # Couche dense étendue\n",
    "    layers.Dropout(0.5),  # Dropout augmenté pour éviter le surapprentissage\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Sortie binaire\n",
    "    layers.Dense(1, activation='sigmoid')  # Sortie binaire pour classification\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 51ms/step - accuracy: 0.6285 - loss: 1.0695 - val_accuracy: 0.5751 - val_loss: 0.7945\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step - accuracy: 0.7411 - loss: 0.5808 - val_accuracy: 0.7477 - val_loss: 0.5187\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - accuracy: 0.7854 - loss: 0.5075 - val_accuracy: 0.7898 - val_loss: 0.4794\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8337 - loss: 0.4107 - val_accuracy: 0.7658 - val_loss: 0.5071\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.8452 - loss: 0.3701 - val_accuracy: 0.7057 - val_loss: 0.6367\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step - accuracy: 0.8591 - loss: 0.3412 - val_accuracy: 0.8288 - val_loss: 0.4126\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step - accuracy: 0.8952 - loss: 0.2710 - val_accuracy: 0.6937 - val_loss: 0.9562\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9069 - loss: 0.2257 - val_accuracy: 0.6441 - val_loss: 1.0345\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9095 - loss: 0.2298 - val_accuracy: 0.8033 - val_loss: 0.4817\n",
      "Epoch 10/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - accuracy: 0.9305 - loss: 0.1946 - val_accuracy: 0.6171 - val_loss: 1.4811\n",
      "Epoch 11/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - accuracy: 0.9438 - loss: 0.1491 - val_accuracy: 0.8198 - val_loss: 0.5263\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v3.1_many_conv\"\n",
    "model.save('./models/model_'+version+'.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropped images from the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Could not find directory ../Model1/images/crop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m last_validation \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../Model1/images/crop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgrayscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\image_dataset_utils.py:232\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, pad_to_aspect_ratio, data_format, verbose)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     seed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1e6\u001b[39m)\n\u001b[1;32m--> 232\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALLOWLIST_FORMATS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\dataset_utils.py:530\u001b[0m, in \u001b[0;36mindex_directory\u001b[1;34m(directory, labels, formats, class_names, shuffle, seed, follow_links, verbose)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     subdirs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(directory, subdir)):\n\u001b[0;32m    532\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m subdir\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:768\u001b[0m, in \u001b[0;36mlist_directory_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of entries contained within a directory.\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03mThe list is in arbitrary order. It does not contain the special entries \".\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;124;03m  errors.NotFoundError if directory doesn't exist\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_directory(path):\n\u001b[1;32m--> 768\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mNotFoundError(\n\u001b[0;32m    769\u001b[0m       node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    770\u001b[0m       op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    771\u001b[0m       message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find directory \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;66;03m# Convert each element to string, since the return values of the\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;66;03m# vector of string should be interpreted as strings, not bytes.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    776\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str_any(filename)\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m _pywrap_file_io\u001b[38;5;241m.\u001b[39mGetChildren(compat\u001b[38;5;241m.\u001b[39mpath_to_bytes(path))\n\u001b[0;32m    778\u001b[0m ]\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Could not find directory ../Model1/images/crop"
     ]
    }
   ],
   "source": [
    "last_validation = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"../Model1/images/crop\",\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 0s - 12ms/step - accuracy: 0.7829 - loss: 0.4972\n",
      "Test Loss: 0.4972\n",
      "Test Accuracy: 0.7829\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
