{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle pour la détermination du type de véhicule\n",
    "\n",
    "### 1. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "\n",
    "ORIGINAL_DATA_DIR = './original_data'\n",
    "PREPROCESSED_DATA_DIR = './preprocessed_data'\n",
    "TRAIN_OUTPUT_FOLDER = PREPROCESSED_DATA_DIR+'/train'\n",
    "VALID_OUTPUT_FOLDER = PREPROCESSED_DATA_DIR+'/valid'\n",
    "TESTS_OUTPUT_FOLDER = PREPROCESSED_DATA_DIR+'/test'\n",
    "MODEL1_IMAGE_SAMPLE_DIR = './model1_image_samples'\n",
    "MODEL_DIR = './resulting_models'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crop provisoire des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def crop_images (dossier_crops, dossier_coordonnees, dossier_images) :\n",
    "    # Obtenir les listes triées des fichiers dans les dossiers\n",
    "    fichiers_images = sorted(os.listdir(dossier_images))\n",
    "    fichiers_coordonnees = sorted(os.listdir(dossier_coordonnees))\n",
    "\n",
    "    # Vérifier si les deux dossiers ont le même nombre de fichiers\n",
    "    if len(fichiers_images) != len(fichiers_coordonnees):\n",
    "        print(\"Le nombre de fichiers dans les deux dossiers ne correspond pas.\")\n",
    "        exit()\n",
    "\n",
    "    # Parcours des images et fichiers de coordonnées\n",
    "    for index, image_file in enumerate(fichiers_images):\n",
    "        if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(dossier_images, image_file)\n",
    "            coord_file = os.path.join(dossier_coordonnees, fichiers_coordonnees[index])\n",
    "            \n",
    "            # Charger l'image\n",
    "            with Image.open(image_path) as img:\n",
    "                img_width, img_height = img.size  # Taille de l'image\n",
    "                # Lire les coordonnées\n",
    "                with open(coord_file, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                    for i, line in enumerate(lines):\n",
    "                        data = line.strip().split()\n",
    "                        \n",
    "                        # Vérifier la validité des données\n",
    "                        if len(data) < 9:\n",
    "                            print(f\"Coordonnées invalides dans {coord_file}, ligne {i + 1}\")\n",
    "                            continue\n",
    "\n",
    "                        img_type = data[-2]\n",
    "\n",
    "                        # Calcul des coordonnées absolues du rectangle (x_min, y_min, x_max, y_max)\n",
    "                        x_min = int(round(float(data[2])))\n",
    "                        y_min = int(round(float(data[1])))\n",
    "                        x_max = int(round(float(data[0])))\n",
    "                        y_max = int(round(float(data[5])))\n",
    "\n",
    "                        # Découper l'image\n",
    "                        cropped_img = img.crop((x_min, y_min, x_max, y_max))\n",
    "                        \n",
    "                        # Sauvegarder l'image cropée\n",
    "                        crop_file_name = f\"{img_type}_{index + 1}_{i + 1}.jpg\"\n",
    "                        crop_path = os.path.join(dossier_crops, crop_file_name)\n",
    "                        cropped_img.save(crop_path, \"JPEG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : Niveau de gris sur tout le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os\n",
    "\n",
    "def change_to_gray (input_folder, output_folder) :\n",
    "    \"\"\"\n",
    "    Transforme une image à l'origine en couleur en niveaux de gris\n",
    "    \"\"\"\n",
    "    for image_name in os.listdir(input_folder) :\n",
    "        img = cv2.imread(f'{input_folder}/{image_name}')\n",
    "        grayFrame = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        output_path = os.path.join(output_folder, image_name)\n",
    "        cv2.imwrite(output_path, grayFrame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : ajout de bruit gaussien sur les images pour simuler une caméra de mauvaise qualité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def add_gaussian_noise(img, mean=0, std=25):\n",
    "    \"\"\"\n",
    "    Ajoute un bruit gaussien à une image.\n",
    "    \"\"\"\n",
    "    # Convertir l'image en tableau numpy\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "\n",
    "    # Générer du bruit gaussien\n",
    "    std = 10\n",
    "    noise = np.random.normal(mean, std, img_array.shape)\n",
    "    noisy_img = img_array + noise\n",
    "\n",
    "    # Limiter les valeurs entre 0 et 255\n",
    "    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
    "    return noisy_img\n",
    "\n",
    "def process_images_with_effects(source_dir, target_dir, apply_blur=True, apply_noise=True):\n",
    "    \"\"\"\n",
    "    Applique flou et bruit aux images redimensionnées.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            save_path = os.path.join(target_dir, filename)\n",
    "\n",
    "            # Charger l'image\n",
    "            with Image.open(file_path) as img:\n",
    "                # Convertir en BGR pour OpenCV\n",
    "                img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Appliquer bruit gaussien\n",
    "                if apply_noise and random.random() > 0.3:  # 50% de chances d'ajouter du bruit\n",
    "                    img_cv = add_gaussian_noise(img_cv, mean=0, std=random.randint(15, 40))\n",
    "\n",
    "                # Convertir en RGB et sauvegarder\n",
    "                final_img = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "                final_img.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : Redimensionnement pour que les images soient environ de la taille de la médiane des tailles des exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def calculate_median_size(image_paths):\n",
    "    \"\"\"Calcul la médiane des tailles (largeur, hauteur) des images.\"\"\"\n",
    "    dimensions = []\n",
    "    for path in image_paths:\n",
    "        with Image.open(path) as img:\n",
    "            dimensions.append(img.size)  # (width, height)\n",
    "    dimensions = np.array(dimensions)\n",
    "    median_width = int(np.median(dimensions[:, 0]))\n",
    "    median_height = int(np.median(dimensions[:, 1]))\n",
    "    return median_width, median_height\n",
    "\n",
    "def add_random_variation(size, variation_percent=0.2):\n",
    "    \"\"\"\n",
    "    Ajoute une variation aléatoire à une taille donnée.\n",
    "    \"\"\"\n",
    "    factor = 1 + random.uniform(-variation_percent, variation_percent)\n",
    "    return int(size * factor)\n",
    "\n",
    "def resize_image_with_aspect_ratio(img, target_width, target_height):\n",
    "    \"\"\"\n",
    "    Redimensionne une image en conservant le ratio d'aspect, en s'adaptant à la taille cible.\n",
    "    \"\"\"\n",
    "    original_width, original_height = img.size\n",
    "    aspect_ratio = original_width / original_height\n",
    "\n",
    "    # Ajuster la largeur et la hauteur pour conserver le ratio\n",
    "    if target_width / target_height > aspect_ratio:\n",
    "        # Ajuste par la hauteur\n",
    "        new_height = target_height\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    else:\n",
    "        # Ajuste par la largeur\n",
    "        new_width = target_width\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "    try :\n",
    "        return img.resize((new_width, new_height), Image.ANTIALIAS)\n",
    "    except :\n",
    "        return img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "def resize_images(source_dir, target_dir, median_size, variation_percent=0.2):\n",
    "    \"\"\"\n",
    "    Redimensionne les images en conservant leur ratio d'aspect, avec des tailles légèrement différentes.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            file_path = os.path.join(source_dir, filename)\n",
    "            with Image.open(file_path) as img:\n",
    "                # Ajouter une variation aléatoire aux dimensions médianes\n",
    "                target_width = add_random_variation(median_size[0], variation_percent)\n",
    "                target_height = add_random_variation(median_size[1], variation_percent)\n",
    "\n",
    "                # Redimensionner en conservant le ratio d'aspect\n",
    "                resized_img = resize_image_with_aspect_ratio(img, target_width, target_height)\n",
    "\n",
    "                # Sauvegarder l'image redimensionnée\n",
    "                save_path = os.path.join(target_dir, filename)\n",
    "                resized_img.save(save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation : UpSampling en retournant les images aléatoirement\n",
    "Tous les exemples uniques de rotation/flip d'image :\n",
    "\n",
    "| Rotation (k) | Flip Horizontal | Flip Vertical |\n",
    "|--------------|-----------------|---------------|\n",
    "| 0            | False           | False         |\n",
    "| 0            | True            | False         |\n",
    "| 0            | False           | True          |\n",
    "| 1            | False           | False         |\n",
    "| 1            | True            | False         |\n",
    "| 2            | False           | False         |\n",
    "| 2            | True            | False         |\n",
    "| 3            | False           | False         |\n",
    "| 3            | True            | False         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "unique_combinations = [\n",
    "    # (0, False, False), ==> Image de base : (nb de flip à 90°, mirroir horizontal, mirroir veritcal)\n",
    "    (0, True, False),\n",
    "    (0, False, True),\n",
    "    (1, False, False),\n",
    "    (1, True, False),\n",
    "    (2, False, False),\n",
    "    (2, True, False),\n",
    "    (3, False, False),\n",
    "    (3, True, False)\n",
    "]\n",
    "\n",
    "def apply_transformations(img, rotation_count, flip_horizontal, flip_vertical):\n",
    "    \"\"\"\n",
    "    Applique une transformation à l'image selon le nombre de rotations et les flips.\n",
    "    \"\"\"\n",
    "    img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "\n",
    "    img = tf.image.rot90(img, k=rotation_count)\n",
    "\n",
    "    # Appliquer les flips\n",
    "    if flip_horizontal:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    if flip_vertical:\n",
    "        img = tf.image.flip_up_down(img)\n",
    "\n",
    "    img_rotated = img.numpy().astype(np.uint8)\n",
    "    return img_rotated\n",
    "\n",
    "def rotate_images (input_folder, output_folder) :\n",
    "    for image_name in os.listdir(input_folder) :\n",
    "        img = cv2.imread(f'{input_folder}/{image_name}')\n",
    "\n",
    "        # On choisit deux transformations aléatoires\n",
    "        random_choice1 = random.choice(unique_combinations)\n",
    "        random_choice2 = random.choice(unique_combinations)\n",
    "\n",
    "        # On vérifie que les choix sont différents\n",
    "        while random_choice1 == random_choice2:\n",
    "            random_choice2 = random.choice(unique_combinations)\n",
    "\n",
    "        rotation_count1, flip_horizontal1, flip_vertical1 = random_choice1\n",
    "        rotation_count2, flip_horizontal2, flip_vertical2 = random_choice2\n",
    "\n",
    "        rotations_list = [apply_transformations(img, rotation_count1, flip_horizontal1, flip_vertical1), \n",
    "                        apply_transformations(img, rotation_count2, flip_horizontal2, flip_vertical2)]\n",
    "        \n",
    "\n",
    "        img_name = os.path.splitext(image_name)[0]\n",
    "        category = img_name.split(\"_\")[0]\n",
    "        output_path = os.path.join(output_folder + f'/{category}/', f\"{img_name}.png\")\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "        for i, rotated_image in enumerate(rotations_list) :\n",
    "            output_image_name = f\"{img_name}_{i+1}.png\"\n",
    "            os.makedirs(f'{output_folder}/{category}', exist_ok=True)\n",
    "            output_path = os.path.join(output_folder + f'/{category}/', output_image_name)\n",
    "            cv2.imwrite(output_path, rotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version finale de la préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None  # Supprime complètement la limite\n",
    "\n",
    "def prepare_data (mode, output_dir) :\n",
    "    print(f'Beginning of the preparation process for {mode} data.\\n')\n",
    "    tmp_output_dir = output_dir+'/tmp'\n",
    "\n",
    "    # Chemins des dossiers\n",
    "    dossier_images = f\"./{ORIGINAL_DATA_DIR}/{mode}/images\"  # Chemin vers le dossier contenant les images\n",
    "    dossier_coordonnees = f\"./{ORIGINAL_DATA_DIR}/{mode}/labelTxt\"  # Chemin vers le dossier contenant les fichiers de coordonnées\n",
    "    \n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(tmp_output_dir, exist_ok=True)\n",
    "    crop_images (tmp_output_dir, dossier_coordonnees, dossier_images)\n",
    "    ### la partie au dessus sert à croper provisoirement. Elle sera à enlever plus tard.\n",
    "\n",
    "    change_to_gray(tmp_output_dir, tmp_output_dir)\n",
    "    print(f'End of the process Color to Gray')\n",
    "\n",
    "    # Appliquer flou et bruit\n",
    "    process_images_with_effects(tmp_output_dir, tmp_output_dir, apply_blur=True, apply_noise=True)\n",
    "    print(f'End of the process Add Effects on Images')\n",
    "\n",
    "    # Images avec tailles définies\n",
    "    defined_images = [os.path.join(MODEL1_IMAGE_SAMPLE_DIR, f) for f in os.listdir(MODEL1_IMAGE_SAMPLE_DIR)\n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "    # Calcul de la taille médiane\n",
    "    median_size = calculate_median_size(defined_images)\n",
    "\n",
    "    # Redimensionnement des images avec une variation aléatoire de ±20 %\n",
    "    resize_images(tmp_output_dir, tmp_output_dir, median_size, variation_percent=0.2)\n",
    "    print(f'End of the process Resize Images')\n",
    "    # shutil.rmtree('./noise_images')\n",
    "\n",
    "    # Génération d'images tournées dans des sens différents\n",
    "    rotate_images (tmp_output_dir, output_dir)\n",
    "    print(f'End of the process Random Turn Images')\n",
    "    shutil.rmtree(tmp_output_dir)\n",
    "\n",
    "    print(f'\\nEnd of the Preparation process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning of the preparation process for train data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n",
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n",
      "Beginning of the preparation process for test data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n",
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n",
      "Beginning of the preparation process for validation data.\n",
      "\n",
      "End of the process Color to Gray\n",
      "End of the process Add Effects on Images\n",
      "End of the process Resize Images\n",
      "End of the process Random Turn Images\n",
      "\n",
      "End of the Preparation process\n"
     ]
    }
   ],
   "source": [
    "prepare_data ('train', f'{TRAIN_OUTPUT_FOLDER}')\n",
    "prepare_data ('test', f'{TESTS_OUTPUT_FOLDER}')\n",
    "prepare_data ('validation', f'{VALID_OUTPUT_FOLDER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2292 files belonging to 2 classes.\n",
      "Found 666 files belonging to 2 classes.\n",
      "Found 327 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Images avec tailles définies\n",
    "defined_images = [os.path.join(MODEL1_IMAGE_SAMPLE_DIR, f) for f in os.listdir(MODEL1_IMAGE_SAMPLE_DIR)\n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "img_width,img_height = calculate_median_size(defined_images)\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_OUTPUT_FOLDER,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    VALID_OUTPUT_FOLDER,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    TESTS_OUTPUT_FOLDER,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    color_mode='grayscale'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True) # Keep best model instead of last\n",
    "early_stopping_cb = EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True) # Prevent overfitting\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Entrée\n",
    "    layers.Input(shape=(img_width, img_height, 1)),\n",
    "\n",
    "    # Bloc convolutionnel 1\n",
    "    layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu'),\n",
    "    layers.BatchNormalization(),  # Ajout d'une normalisation\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Bloc convolutionnel 2\n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # Bloc convolutionnel 3\n",
    "    layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # Bloc convolutionnel 4 (ajouté pour extraire davantage de caractéristiques complexes)\n",
    "    layers.Conv2D(filters=256, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "\n",
    "    # Bloc convolutionnel 5 (couche supplémentaire avec une taille de noyau différente)\n",
    "    layers.Conv2D(filters=512, kernel_size=(2, 2), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "\n",
    "    # Aplatissement des caractéristiques\n",
    "    layers.Flatten(),\n",
    "\n",
    "    # Couches denses\n",
    "    layers.Dense(512, activation='relu'),  # Couche dense étendue\n",
    "    layers.Dropout(0.5),  # Dropout augmenté pour éviter le surapprentissage\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "\n",
    "    # Sortie binaire\n",
    "    layers.Dense(1, activation='sigmoid')  # Sortie binaire pour classification\n",
    "])\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">38</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">786,944</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m38\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_12          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_13          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m786,944\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,502,981</span> (20.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,502,981\u001b[0m (20.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,833,665</span> (6.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,833,665\u001b[0m (6.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> (7.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,984\u001b[0m (7.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,667,332</span> (13.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,667,332\u001b[0m (13.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 222ms/step - accuracy: 0.6383 - loss: 1.0819 - val_accuracy: 0.4850 - val_loss: 1.1439\n",
      "Epoch 2/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 186ms/step - accuracy: 0.7338 - loss: 0.6147 - val_accuracy: 0.6997 - val_loss: 0.5564\n",
      "Epoch 3/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 176ms/step - accuracy: 0.7454 - loss: 0.5880 - val_accuracy: 0.6652 - val_loss: 0.6202\n",
      "Epoch 4/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 220ms/step - accuracy: 0.7909 - loss: 0.5056 - val_accuracy: 0.7087 - val_loss: 0.5485\n",
      "Epoch 5/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 180ms/step - accuracy: 0.8261 - loss: 0.3981 - val_accuracy: 0.7628 - val_loss: 0.5659\n",
      "Epoch 6/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 174ms/step - accuracy: 0.8524 - loss: 0.3592 - val_accuracy: 0.7838 - val_loss: 0.6346\n",
      "Epoch 7/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 171ms/step - accuracy: 0.8716 - loss: 0.2914 - val_accuracy: 0.7072 - val_loss: 0.9377\n",
      "Epoch 8/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.8886 - loss: 0.2701 - val_accuracy: 0.6216 - val_loss: 3.8294\n",
      "Epoch 9/15\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 179ms/step - accuracy: 0.9052 - loss: 0.2450 - val_accuracy: 0.7928 - val_loss: 0.6200\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping_cb, checkpoint_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v3.1_many_conv\"\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "model.save(f'{MODEL_DIR}/model_'+version+'.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cropped images from the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_validation = tf.keras.utils.image_dataset_from_directory(\n",
    "#     \"../Model1/images/crop\",\n",
    "#     image_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     label_mode='int',\n",
    "#     color_mode='grayscale'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 - 1s - 50ms/step - accuracy: 0.7034 - loss: 0.6964\n",
      "Test Loss: 0.6964\n",
      "Test Accuracy: 0.7034\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset, verbose=2)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
